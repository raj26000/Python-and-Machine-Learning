# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b34cFhZ8ReSjBVRzufOuYrqPIvoqcmeH

# I. Importing Data and Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
from sklearn.metrics import accuracy_score, make_scorer
from google.colab import files

train_data = pd.read_csv('https://raw.githubusercontent.com/raj26000/Python-and-Machine-Learning/main/DPhi%20DataSprint%20%2334/Train_data.csv')
test_data = pd.read_csv('https://raw.githubusercontent.com/raj26000/Python-and-Machine-Learning/main/DPhi%20DataSprint%20%2334/Test_data.csv')
train_data

"""# II. Exploratory Data Analysis (EDA)

*   **Check Missing Values**
"""

train_data.isnull().sum()

test_data.isnull().sum()

"""

*   **Check for Class Imbalance**

"""

sns.set()
sns.countplot(x = train_data['Target'])

"""

*   **Feature Selection with Kendall-Tau Correlation Coefficient**

"""

corr = train_data.corr(method='kendall')
plt.figure(figsize=(15,12))
sns.heatmap(corr, annot=True)

#Drop columns at 0.9 correlation threshold only.
y_train = train_data['Target']
X_train = train_data.drop(['Target', 'PTS'], axis=1)
test_data = test_data.drop(['PTS'], axis=1)

"""

*   **Visualizing Distribution of Numerical Variables**

"""

X_train[X_train.columns].hist(layout=(5,4), bins=20, figsize=(20,15))

"""

*   **Feature Standardization**

"""

scaler = StandardScaler()
train_sc = scaler.fit_transform(X_train)
test_sc = scaler.transform(test_data)
train_data = pd.DataFrame(data = train_sc, columns = X_train.columns)
test_data = pd.DataFrame(data = test_sc, columns = test_data.columns)

"""# III. Model Training with LightGBM"""

#Similar results were obtained with RandomForestClassifier too. GridSearch tuning not done due to infinite submission limit,
#and to study impact of each feature on score individually.
import xgboost as xgb
best_clf = xgb.XGBClassifier(n_estimators=600, max_depth=5)
best_clf.fit(train_data, y_train)
y_pred = best_clf.predict(test_data)

df = pd.DataFrame()
df['prediction'] = y_pred
df.to_csv('dphi_34.csv', index=False)
files.download('dphi_34.csv')
'''
Final Accuracy Score: RandomForest ~ 0.9856
                      LightGBM ~ 0.987387
                      Catboost ~ 0.9423
                      SVM ~ 0.92
                      LogisticRegression ~ 0.7 
                      KNN  ~ 0.77       '''

